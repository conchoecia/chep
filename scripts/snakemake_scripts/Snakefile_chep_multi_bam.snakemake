"""
Snakemake workflow to process multiple BAM files for CHEP analysis.
Each BAM is processed independently, then results are merged.

Usage:
    snakemake -s Snakefile_chep_multi_bam \
        --config ref=reference.fa bam_list=bam_list.txt output=merged_output.txt \
        -j 100 \
        --cluster "sbatch -c 1 --mem=4G -t 2:00:00"
"""

import os
from pathlib import Path

# Get config parameters
REF = config.get("ref", "reference.fa")
BAM_LIST = config.get("bam_list", "bam_list.txt")
OUTPUT = config.get("output", "chep_merged.txt")
WORKDIR = config.get("workdir", "chep_work")
GENOME_SIZE_GB = config.get("genome_size_gb", 3.0)  # Default 3Gbp genome

# Get the directory containing this Snakefile
SNAKEFILE_DIR = Path(workflow.basedir)
# CHEP bin directory (two levels up from scripts/snakemake_scripts/)
CHEP_BIN = SNAKEFILE_DIR.parent.parent / "bin"
CHEP_SCRIPTS = SNAKEFILE_DIR.parent

# Path to executables
CHEP_PILEUP_TO_ARRAY = CHEP_BIN / "chep_pileup_to_array"
MERGE_CHEP3D = CHEP_SCRIPTS / "merge_chep3d.py"

# Read BAM file list
with open(BAM_LIST) as f:
    BAMS = [line.strip() for line in f if line.strip()]

# Create BAM IDs (sanitized filenames for output)
BAM_IDS = [f"bam_{i:04d}" for i in range(len(BAMS))]

# Create mapping of IDs to BAM paths
BAM_DICT = dict(zip(BAM_IDS, BAMS))

# Define output directory
os.makedirs(WORKDIR, exist_ok=True)

def get_bam_size_mb(wildcards):
    """Get BAM file size in MB"""
    bam_path = BAM_DICT[wildcards.bam_id]
    try:
        size_bytes = os.path.getsize(bam_path)
        return size_bytes / (1024 * 1024)
    except:
        # If we can't get size, return a conservative default
        return 500  # 500MB default

def estimate_mem_mb(wildcards, attempt=1):
    """
    Estimate memory needed based on BAM size.
    Empirical data: 283MB BAM used 412MB RAM with 3Gbp genome
    Ratio: ~1.5x BAM size in MB
    Add buffer for safety and retry attempts
    """
    bam_size_mb = get_bam_size_mb(wildcards)
    base_mem = max(1000, int(bam_size_mb * 1.5))  # At least 1GB
    # Increase memory on retry attempts
    return base_mem * attempt

def estimate_runtime(wildcards, attempt=1):
    """
    Estimate runtime based on BAM size.
    Empirical data: 283MB BAM took ~13 minutes with 3Gbp genome
    Rate: ~0.046 minutes per MB of BAM (~22 MB/min throughput)
    Add buffer for safety and retry attempts
    """
    bam_size_mb = get_bam_size_mb(wildcards)
    base_time = max(15, int(bam_size_mb * 0.046 * 1.5))  # At least 15 min, 1.5x safety factor
    # Increase time on retry attempts
    return base_time * attempt

def estimate_disk_mb(wildcards, attempt=1):
    """
    Estimate local disk space needed.
    Need space for BAM + index (~1.1x BAM size)
    """
    bam_size_mb = get_bam_size_mb(wildcards)
    return int(bam_size_mb * 1.2)  # 20% buffer for index

def estimate_merge_mem_mb(wildcards, input, attempt=1):
    """
    Estimate memory for merge based on number of files.
    Each file is small (just counts), but need some overhead.
    """
    num_files = len(input)
    base_mem = 2000 + (num_files * 2)  # 2GB base + 2MB per file
    return min(base_mem * attempt, 16000)  # Cap at 16GB

def estimate_merge_runtime(wildcards, input, attempt=1):
    """
    Estimate merge runtime based on number of files.
    This is I/O bound, mainly reading many small files.
    """
    num_files = len(input)
    base_time = max(10, int(num_files * 0.05))  # ~3 seconds per file, minimum 10 min
    return base_time * attempt

rule all:
    input:
        OUTPUT

rule process_single_bam:
    """
    Process a single BAM file using local tmpdir for I/O efficiency.
    Copies BAM and index to local tmpdir, runs mpileup, then cleans up.

    Resources are dynamically allocated based on BAM file size:
    - Memory: ~1.5x BAM size (empirically validated)
    - Runtime: ~0.046 min/MB of BAM (from 283MBâ†’13min benchmark)
    - Disk: 1.2x BAM size for local copy + index
    """
    output:
        f"{WORKDIR}/{{bam_id}}.chep3d.txt"
    params:
        bam_path = lambda wildcards: BAM_DICT[wildcards.bam_id],
        ref = REF
    threads: 1
    resources:
        mem_mb = estimate_mem_mb,
        runtime = estimate_runtime,
        disk_mb = estimate_disk_mb
    shell:
        """
        # Create local temporary directory
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d $TMPDIR/chep_XXXXXX)

        echo "Working directory: $LOCAL_TMP"
        echo "Processing BAM: {params.bam_path}"

        # Copy BAM and its index to local tmpdir with unique name
        # Use wildcards.bam_id to ensure unique filename even if source BAMs have same basename
        LOCAL_BAM=$LOCAL_TMP/{wildcards.bam_id}.bam
        cp {params.bam_path} $LOCAL_BAM

        # Copy index file (.bam.bai)
        if [ -f {params.bam_path}.bai ]; then
            cp {params.bam_path}.bai $LOCAL_BAM.bai
        else
            echo "Warning: Index file not found, will create one"
            samtools index $LOCAL_BAM
        fi

        # Run mpileup on the local copy
        samtools mpileup -B -Q 0 -q 0 -A -R -f {params.ref} $LOCAL_BAM | \
            {CHEP_PILEUP_TO_ARRAY} > {output}

        # Cleanup
        rm -rf $LOCAL_TMP

        echo "Completed: {wildcards.bam_id}"
        """

rule merge_all:
    """
    Merge all individual chep3d outputs into final result.
    Resources scale with number of input files.
    """
    input:
        expand(f"{WORKDIR}/{bam_id}.chep3d.txt", bam_id = BAM_IDS)
    output:
        OUTPUT
    threads: 1
    resources:
        mem_mb = estimate_merge_mem_mb,
        runtime = estimate_merge_runtime
    shell:
        """
        echo "Merging input files..."
        python3 {MERGE_CHEP3D} {input} > {output}
        echo "Merge complete: {output}"
        """

rule clean:
    """
    Clean up intermediate files.
    """
    shell:
        f"rm -rf {WORKDIR}"
