"""
Snakemake workflow for hierarchical BAM merging (v3).
Strategy: Two-level merge using sqrt(N) batches for balanced merging.

Level 1: Merge N BAMs into sqrt(N) batch BAMs
Level 2: Merge sqrt(N) batch BAMs into 1 final BAM

Usage:
    snakemake -s Snakefile_chep_multi_bam_v3.snakemake \
        --config ref=reference.fa bam_list=bam_list.txt output=merged.bam \
        --executor slurm --jobs 100
"""

import os
import math
from pathlib import Path

# Get the directory containing this Snakefile
SNAKEFILE_DIR = Path(workflow.basedir)

# Get config parameters
REF = config.get("ref", "reference.fa")
BAM_LIST = config.get("bam_list", "bam_list.txt")
OUTPUT = config.get("output", "merged_all.bam")
WORKDIR = config.get("workdir", "chep_merge_work")
CHEP_BIN = config.get("chep_bin", str(SNAKEFILE_DIR.parent.parent / "bin" ))


# Read BAM file list
with open(BAM_LIST) as f:
    BAMS = [line.strip() for line in f if line.strip()]

NUM_BAMS = len(BAMS)

# Calculate optimal batch size (square root of total BAMs)
BATCH_SIZE = max(2, int(math.sqrt(NUM_BAMS)))
NUM_BATCHES = math.ceil(NUM_BAMS / BATCH_SIZE)

print(f"Total BAMs: {NUM_BAMS}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Number of batches: {NUM_BATCHES}")

BAM_IDS = [f"bam_{i:04d}" for i in range(NUM_BAMS)]
BATCH_IDS = [f"batch_{i:04d}" for i in range(NUM_BATCHES)]

def get_bams_for_batch(batch_id):
    """Get list of BAM files for a given batch"""
    batch_num = int(batch_id.split("_")[1])
    start_idx = batch_num * BATCH_SIZE
    end_idx = min(start_idx + BATCH_SIZE, NUM_BAMS)
    return BAMS[start_idx:end_idx]

def estimate_final_merge_runtime(attempt=1):
    """
    Estimate runtime for final merge based on number of BAMs.
    
    Empirical observation: 768 BAMs takes ~9 hours (540 minutes) for merge only.
    (Indexing is done separately in its own rule)
    
    This includes:
    - Copying batch BAMs to tmpdir
    - Merging all batch BAMs
    - Copying back to network storage
    
    Runtime scales approximately linearly with number of BAMs.
    Formula: (NUM_BAMS / 768) × 540 minutes × 1.4 safety margin
    """
    # Base runtime for 768 BAMs is 540 minutes (9 hours)
    base_runtime = (NUM_BAMS / 768.0) * 540
    
    # Add 40% safety margin
    safe_runtime = int(base_runtime * 1.4)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_index_runtime(attempt=1):
    """
    Estimate runtime for indexing final merged BAM based on number of BAMs.
    
    Empirical observation: Indexing 768 merged BAMs takes longer than initially expected.
    
    Runtime scales with the size of the final merged BAM, which is roughly
    proportional to the number of input BAMs.
    """
    # Base runtime for 768 BAMs is ~360 minutes (6 hours) for indexing
    base_runtime = (NUM_BAMS / 768.0) * 360
    
    # Add 50% safety margin
    safe_runtime = int(base_runtime * 1.5)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_split_runtime(bam_path, attempt=1):
    """
    Estimate runtime for splitting BAM by chromosome based on file size.
    
    Empirical observation: 123GB BAM file failed at 4 hours (240 min) on first attempt,
    succeeded on second attempt (480 min = 8 hours).
    
    Runtime scales with BAM file size.
    Formula: (file_size_gb / 123) × 480 minutes × 1.3 safety margin
    """
    import os
    
    # Get file size in GB
    file_size_bytes = os.path.getsize(bam_path)
    file_size_gb = file_size_bytes / (1024**3)
    
    # Base runtime for 123GB is 480 minutes (8 hours)
    base_runtime = (file_size_gb / 123.0) * 480
    
    # Add 30% safety margin
    safe_runtime = int(base_runtime * 1.3)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_mpileup_runtime(bam_path, attempt=1):
    """
    Estimate runtime for mpileup based on BAM file size.
    
    Empirical observations:
    - 6.7 GB BAM: >120 minutes (failed at 2 hours)
    - 809 KB (0.0008 GB) BAM: ~1 minute
    
    Runtime scales with BAM file size.
    Conservative estimate: ~20 minutes per GB with 50% safety margin
    Minimum runtime: 5 minutes for very small BAMs
    """
    import os
    
    # Get file size in GB
    file_size_bytes = os.path.getsize(bam_path)
    file_size_gb = file_size_bytes / (1024**3)
    
    # Base: 20 minutes per GB (conservative estimate)
    base_runtime = file_size_gb * 20
    
    # Add 50% safety margin
    safe_runtime = int(base_runtime * 1.5)
    
    # Minimum 5 minutes for very small BAMs
    safe_runtime = max(5, safe_runtime)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_chep_runtime(pileup_path, attempt=1):
    """
    Estimate runtime for CHEP processing based on pileup file size.
    
    Runtime scales with pileup file size.
    Conservative estimate: ~3 minutes per 100 MB with 50% safety margin
    Minimum runtime: 10 minutes for very small pileups
    """
    import os
    
    # Get file size in MB
    file_size_bytes = os.path.getsize(pileup_path)
    file_size_mb = file_size_bytes / (1024**2)
    
    # Base: 3 minutes per 100 MB (3x increase from original)
    base_runtime = (file_size_mb / 100.0) * 3
    
    # Add 50% safety margin
    safe_runtime = int(base_runtime * 1.5)
    
    # Minimum 10 minutes for very small pileups
    safe_runtime = max(10, safe_runtime)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_depth_stats_runtime(pileup_path, attempt=1):
    """
    Estimate runtime for windowed depth stats calculation based on pileup.gz size.
    
    Empirical: 1.4GB file failed at 20 minutes, 2.9GB files completed in <20 minutes
    This suggests processing speed varies significantly, possibly due to compression
    ratio, line count, or other factors.
    
    Conservative estimate: 15 minutes per GB with 2x safety margin
    Minimum runtime: 30 minutes to handle edge cases
    """
    import os
    
    # Get file size in GB
    file_size_bytes = os.path.getsize(pileup_path)
    file_size_gb = file_size_bytes / (1024**3)
    
    # Base: 15 minutes per GB
    base_runtime = file_size_gb * 15
    
    # Add 2x safety margin (processing time can vary a lot)
    safe_runtime = int(base_runtime * 2)
    
    # Minimum 30 minutes for edge cases
    safe_runtime = max(30, safe_runtime)
    
    # Scale with retry attempts
    return safe_runtime * attempt

# Create work directories
os.makedirs(WORKDIR, exist_ok=True)
os.makedirs(WORKDIR + "/batch_bams", exist_ok=True)

# Wildcard constraints
wildcard_constraints:
    batch_id = r"batch_\d+",
    chrom = r"[A-Za-z0-9_.\-]+"

def get_chrom_pileups(wildcards):
    """
    Aggregation function to get list of chromosome pileup files.
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output[0]
    outdir = WORKDIR + "/split_by_chrom"

    # Return list of pileup files to generate
    return expand(WORKDIR + "/pileups/{chrom}.pileup.gz",
                  chrom=glob_wildcards(os.path.join(outdir, "{chrom}.bam")).chrom)

def get_chrom_outputs(wildcards):
    """
    Aggregation function to get list of CHEP output files (one per chromosome).
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output[0]
    outdir = WORKDIR + "/split_by_chrom"

    # Return list of CHEP output files to generate
    return expand(WORKDIR + "/chep_outputs/{chrom}.chep.txt",
                  chrom=glob_wildcards(os.path.join(outdir, "{chrom}.bam")).chrom)

def get_depth_stats(wildcards):
    """
    Aggregation function to get list of depth stats files (one per chromosome).
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output[0]
    outdir = WORKDIR + "/split_by_chrom"

    # Return list of depth stats files to generate
    return expand(WORKDIR + "/depth_stats/{chrom}_depth_stats.tsv",
                  chrom=glob_wildcards(os.path.join(outdir, "{chrom}.bam")).chrom)

def get_variant_calls(wildcards):
    """
    Aggregation function to get list of variant VCF files (one per chromosome).
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output[0]
    outdir = WORKDIR + "/split_by_chrom"

    # Return list of VCF files to generate
    return expand(WORKDIR + "/variants/{chrom}.vcf.gz",
                  chrom=glob_wildcards(os.path.join(outdir, "{chrom}.bam")).chrom)

def get_filtered_snps(wildcards):
    """
    Aggregation function to get list of filtered SNP VCF files (one per chromosome).
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output[0]
    outdir = WORKDIR + "/split_by_chrom"

    # Return list of filtered SNP VCF files to generate
    return expand(WORKDIR + "/variants/{chrom}.snps.q20.vcf.gz",
                  chrom=glob_wildcards(os.path.join(outdir, "{chrom}.bam")).chrom)

rule all:
    input:
        WORKDIR + "/chep_combined_matrix.txt",
        WORKDIR + "/plots/plots_marginal_het_het_plot.pdf",
        get_depth_stats,
        get_filtered_snps


rule merge_batch_bams:
    """
    Merge a batch of BAMs into a single batch BAM.
    Uses local tmpdir for fast I/O.
    """
    input:
        bams = lambda wc: get_bams_for_batch(wc.batch_id)
    output:
        bam = WORKDIR + "/batch_bams/{batch_id}.bam",
        bai = WORKDIR + "/batch_bams/{batch_id}.bam.bai"
    params:
        batch_id = "{batch_id}",
        num_bams = lambda wc: len(get_bams_for_batch(wc.batch_id))
    threads: 4
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: max(8000, len(get_bams_for_batch(wildcards.batch_id)) * 500) * attempt,
        runtime = lambda wildcards, attempt: 120 * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_batch_merge_XXXXXX")
        mkdir -p "$LOCAL_TMP/bams"

        echo "Merging {params.batch_id} with {params.num_bams} BAMs..."
        echo "Working directory: $LOCAL_TMP"

        # Copy BAMs to local tmpdir with unique names
        BAM_LIST="$LOCAL_TMP/bam_list.txt"
        counter=0
        for bam in {input.bams}; do
            counter=$((counter + 1))
            LOCAL_BAM="$LOCAL_TMP/bams/bam_${{counter}}.bam"
            echo "  Copying BAM $counter to tmpdir..."
            cp "$bam" "$LOCAL_BAM"

            # Copy or create index
            if [ -f "${{bam}}.bai" ]; then
                cp "${{bam}}.bai" "${{LOCAL_BAM}}.bai"
            else
                echo "  Creating index for BAM $counter..."
                samtools index "$LOCAL_BAM"
            fi

            echo "$LOCAL_BAM" >> "$BAM_LIST"
        done

        # Merge BAMs
        echo "Merging {params.num_bams} BAMs in {params.batch_id}..."
        LOCAL_OUT="$LOCAL_TMP/merged.bam"
        samtools merge -@ {threads} -b "$BAM_LIST" "$LOCAL_OUT"

        # Index the merged BAM
        echo "Indexing merged BAM..."
        samtools index "$LOCAL_OUT"

        # Copy results back
        echo "Copying merged BAM to output directory..."
        mkdir -p $(dirname {output.bam})
        cp "$LOCAL_OUT" {output.bam}
        cp "${{LOCAL_OUT}}.bai" {output.bai}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Completed merge for {params.batch_id}"
        """

rule merge_all_batches:
    """
    Merge all batch BAMs into final output BAM.
    Uses local tmpdir for fast I/O.
    Does NOT index - that's done in a separate rule.
    """
    input:
        bams = expand(WORKDIR + "/batch_bams/{batch_id}.bam", batch_id=BATCH_IDS),
        bais = expand(WORKDIR + "/batch_bams/{batch_id}.bam.bai", batch_id=BATCH_IDS)
    output:
        bam = WORKDIR + "/merged_all.bam"
    params:
        num_batches = NUM_BATCHES
    threads: 8
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: max(16000, NUM_BATCHES * 1000) * attempt,
        runtime = lambda wildcards, attempt: estimate_final_merge_runtime() * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_final_merge_XXXXXX")
        mkdir -p "$LOCAL_TMP/bams"

        echo "Merging {params.num_batches} batch BAMs into final output..."
        echo "Working directory: $LOCAL_TMP"

        # Copy batch BAMs to local tmpdir
        BAM_LIST="$LOCAL_TMP/bam_list.txt"
        counter=0
        for bam in {input.bams}; do
            counter=$((counter + 1))
            LOCAL_BAM="$LOCAL_TMP/bams/batch_${{counter}}.bam"
            echo "  Copying batch BAM $counter to tmpdir..."
            cp "$bam" "$LOCAL_BAM"
            cp "${{bam}}.bai" "${{LOCAL_BAM}}.bai"
            echo "$LOCAL_BAM" >> "$BAM_LIST"
        done

        # Final merge (NO INDEXING - that's done separately)
        echo "Merging {params.num_batches} batch BAMs..."
        LOCAL_OUT="$LOCAL_TMP/final_merged.bam"
        samtools merge -@ {threads} -b "$BAM_LIST" "$LOCAL_OUT"

        # Copy results back
        echo "Copying final merged BAM to output directory..."
        mkdir -p $(dirname {output.bam})
        cp "$LOCAL_OUT" {output.bam}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Final merge complete: {output.bam}"
        """

rule index_final_bam:
    """
    Index the final merged BAM.
    Separate from merge to allow better resource allocation.
    Note: samtools index uses 2 hidden threads, so we reserve 10 threads
    from SLURM but only tell samtools to use 8.
    """
    input:
        bam = WORKDIR + "/merged_all.bam"
    output:
        bai = WORKDIR + "/merged_all.bam.bai"
    threads: 10
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 4000 * attempt,
        runtime = lambda wildcards, attempt: estimate_index_runtime() * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_index_XXXXXX")
        
        echo "Indexing final merged BAM: {input.bam}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy BAM to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/merged.bam"
        echo "Copying BAM to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        
        # Index the BAM (use 8 threads, SLURM reserves 10 for the 2 hidden threads)
        echo "Indexing BAM..."
        samtools index -@ 8 "$LOCAL_BAM"
        
        # Copy index back to output
        echo "Copying index to output directory..."
        mkdir -p $(dirname {output.bai})
        cp "${{LOCAL_BAM}}.bai" {output.bai}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Indexing complete: {output.bai}"
        """

checkpoint split_bam_by_chromosome:
    """
    Split the final merged BAM by chromosome.
    Uses GNU parallel to split in parallel (4 jobs at a time).
    Only processes chromosomes >= 50kbp in length.
    Each chromosome gets its own BAM file and index.
    
    Note: Runs 4 parallel jobs, each using samtools with 4 threads = 16 threads total.
    With observed ~20-30% CPU utilization, we reserve 20 threads to be safe.
    """
    input:
        bam = WORKDIR + "/merged_all.bam",
        bai = WORKDIR + "/merged_all.bam.bai"
    output:
        outdir = directory(WORKDIR + "/split_by_chrom"),
        done   = WORKDIR + "/split_by_chrom/split.done"
    threads: 20
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = lambda wildcards, attempt, input: estimate_split_runtime(input.bam, attempt)
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_split_XXXXXX")
        
        echo "Splitting BAM by chromosome (only >= 50kbp): {input.bam}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy BAM and index to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/merged.bam"
        echo "Copying BAM and index to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        cp {input.bai} "${{LOCAL_BAM}}.bai"
        
        # Create local output directory
        LOCAL_OUTDIR="$LOCAL_TMP/split_chroms"
        mkdir -p "$LOCAL_OUTDIR"
        
        # Split by chromosome using parallel (only chromosomes >= 50000 bp)
        # Run 4 parallel jobs, each using 4 threads (16 threads total)
        echo "Splitting by chromosome (4 parallel jobs, filtering >= 50kbp)..."
        samtools idxstats "$LOCAL_BAM" | awk '$1!="*" && $2>=50000{{print $1}}' | \
          parallel -j 4 "samtools view -@ 4 -b $LOCAL_BAM {{}} -o $LOCAL_OUTDIR/{{}}.bam && samtools index -@ 4 $LOCAL_OUTDIR/{{}}.bam"
        
        # Copy results back to output directory
        echo "Copying split BAMs to output directory..."
        mkdir -p {output.outdir}
        cp "$LOCAL_OUTDIR"/*.bam {output.outdir}/
        cp "$LOCAL_OUTDIR"/*.bai {output.outdir}/

        # Create done marker
        touch {output.done}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Chromosome splitting complete: {output.outdir}"
        """

rule mpileup_chrom:
    """
    Run samtools mpileup on a single chromosome BAM file.
    Output gzipped pileup text.
    Uses local tmpdir for I/O efficiency.
    
    Each chromosome is processed independently for parallelization.
    """
    input:
        bam = WORKDIR + "/split_by_chrom/{chrom}.bam",
        bai = WORKDIR + "/split_by_chrom/{chrom}.bam.bai",
        ref = REF
    output:
        pileup = WORKDIR + "/pileups/{chrom}.pileup.gz"
    threads: 1
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 4000 * attempt,
        runtime = lambda wildcards, attempt, input: estimate_mpileup_runtime(input.bam, attempt)
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_mpileup_XXXXXX")

        echo "Running mpileup on chromosome {wildcards.chrom}"
        echo "Working directory: $LOCAL_TMP"

        # Copy BAM and index to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/{wildcards.chrom}.bam"
        echo "Copying BAM and index to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        cp {input.bai} "${{LOCAL_BAM}}.bai"

        # Run mpileup and pipe directly to gzip in tmpdir
        LOCAL_OUT="$LOCAL_TMP/{wildcards.chrom}.pileup.gz"
        echo "Generating pileup..."
        samtools mpileup -B -Q 0 -q 0 -A -R -f {input.ref} "$LOCAL_BAM" | gzip > "$LOCAL_OUT"

        # Copy gzipped result back to output directory
        echo "Copying pileup to output directory..."
        mkdir -p $(dirname {output.pileup})
        cp "$LOCAL_OUT" {output.pileup}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Completed mpileup for chromosome {wildcards.chrom}"
        """

rule call_variants_chrom:
    """
    Call variants from a single chromosome BAM using bcftools.
    Uses bcftools mpileup piped into bcftools call for efficient variant calling.
    
    Each chromosome is processed independently for parallelization.
    Outputs compressed VCF (BCF format) for efficiency.
    
    Note: bcftools mpileup is much faster than samtools mpileup, so we use
    0.5× the samtools mpileup estimate (1/4 of the previous 2× estimate).
    """
    input:
        bam = WORKDIR + "/split_by_chrom/{chrom}.bam",
        bai = WORKDIR + "/split_by_chrom/{chrom}.bam.bai",
        ref = REF
    output:
        vcf = WORKDIR + "/variants/{chrom}.vcf.gz",
        csi = WORKDIR + "/variants/{chrom}.vcf.gz.csi"
    threads: 2
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = lambda wildcards, attempt, input: int(estimate_mpileup_runtime(input.bam, attempt) * 2)
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_variants_XXXXXX")
        
        echo "Calling variants for chromosome {wildcards.chrom}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy BAM and index to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/{wildcards.chrom}.bam"
        echo "Copying BAM and index to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        cp {input.bai} "${{LOCAL_BAM}}.bai"
        
        # Run bcftools mpileup piped into bcftools call
        LOCAL_VCF="$LOCAL_TMP/{wildcards.chrom}.vcf.gz"
        echo "Running bcftools mpileup and call..."
        bcftools mpileup -f {input.ref} --ignore-RG -Ou -A "$LOCAL_BAM" | \
          bcftools call -mv -Oz --threads {threads} -o "$LOCAL_VCF"
        
        # Index the VCF
        echo "Indexing VCF..."
        bcftools index "$LOCAL_VCF"
        
        # Copy results back to output directory
        echo "Copying VCF and index to output directory..."
        mkdir -p $(dirname {output.vcf})
        cp "$LOCAL_VCF" {output.vcf}
        cp "${{LOCAL_VCF}}.csi" {output.csi}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Completed variant calling for chromosome {wildcards.chrom}"
        """

rule filter_snps:
    """
    Filter variants to keep only high-quality SNPs (QUAL > 20).
    Takes raw variant calls and outputs filtered SNP-only VCF.
    Uses local tmpdir for fast I/O to avoid locking up network storage.
    """
    input:
        vcf = WORKDIR + "/variants/{chrom}.vcf.gz",
        csi = WORKDIR + "/variants/{chrom}.vcf.gz.csi"
    output:
        vcf = WORKDIR + "/variants/{chrom}.snps.q20.vcf.gz",
        csi = WORKDIR + "/variants/{chrom}.snps.q20.vcf.gz.csi"
    threads: 2
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 2000 * attempt,
        runtime = lambda wildcards, attempt: 10 * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_filter_snps_XXXXXX")
        
        echo "Filtering SNPs with QUAL > 20 for chromosome {wildcards.chrom}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy input VCF and index to tmpdir
        LOCAL_VCF="$LOCAL_TMP/{wildcards.chrom}.vcf.gz"
        echo "Copying VCF and index to tmpdir..."
        cp {input.vcf} "$LOCAL_VCF"
        cp {input.csi} "${{LOCAL_VCF}}.csi"
        
        # Filter to SNPs only with QUAL > 20 in tmpdir
        LOCAL_OUT="$LOCAL_TMP/{wildcards.chrom}.snps.q20.vcf.gz"
        echo "Filtering variants..."
        bcftools view -v snps -i 'QUAL>20' "$LOCAL_VCF" -Oz --threads {threads} -o "$LOCAL_OUT"
        
        # Index the filtered VCF
        echo "Indexing filtered VCF..."
        bcftools index "$LOCAL_OUT"
        
        # Copy results back to output directory
        echo "Copying filtered VCF and index to output directory..."
        mkdir -p $(dirname {output.vcf})
        cp "$LOCAL_OUT" {output.vcf}
        cp "${{LOCAL_OUT}}.csi" {output.csi}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Completed SNP filtering for chromosome {wildcards.chrom}"
        """

rule process_pileup_with_chep:
    """
    Process a single chromosome pileup with CHEP pileup_to_array.
    Each chromosome is processed independently for parallelization.
    
    Uses local tmpdir to avoid slow I/O on network storage.
    """
    input:
        pileup = WORKDIR + "/pileups/{chrom}.pileup.gz"
    output:
        matrix = WORKDIR + "/chep_outputs/{chrom}.chep.txt"
    threads: 1
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = lambda wildcards, attempt, input: estimate_chep_runtime(input.pileup, attempt)
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_process_XXXXXX")
        
        echo "Processing pileup for chromosome {wildcards.chrom} with CHEP"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy pileup file to local tmpdir for fast I/O
        echo "Copying pileup to tmpdir..."
        LOCAL_PILEUP="$LOCAL_TMP/{wildcards.chrom}.pileup.gz"
        cp {input.pileup} "$LOCAL_PILEUP"
        
        echo "Running CHEP on {wildcards.chrom}..."
        
        # Process with CHEP
        LOCAL_OUT="$LOCAL_TMP/{wildcards.chrom}.chep.txt"
        zcat "$LOCAL_PILEUP" | {CHEP_BIN}/chep_pileup_to_array > "$LOCAL_OUT"
        
        # Copy result back
        echo "Copying output to final location..."
        mkdir -p $(dirname {output.matrix})
        cp "$LOCAL_OUT" {output.matrix}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "CHEP processing complete for {wildcards.chrom}: {output.matrix}"
        """

rule sum_chep_matrices:
    """
    Sum all chromosome CHEP matrices into a single combined matrix.
    Each CHEP file contains a triangular matrix format where each line is:
    row col count
    
    Missing cells with 0 counts have no lines in the file.
    This rule sums the counts for matching (row, col) pairs across all chromosomes.
    """
    input:
        matrices = get_chrom_outputs
    output:
        combined = WORKDIR + "/chep_combined_matrix.txt"
    threads: 1
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 16000 * attempt,
        runtime = lambda wildcards, attempt: 60 * attempt
    run:
        from collections import defaultdict
        import time
        
        total_matrices = len(input.matrices)
        print(f"Summing {total_matrices} CHEP matrices")
        print(f"Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Dictionary to store summed counts: (row, col) -> count
        matrix_sum = defaultdict(int)
        
        # Read and sum all matrices
        start_time = time.time()
        import sys
        
        for idx, matrix_file in enumerate(input.matrices, 1):
            # Read the file
            with open(matrix_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split()
                    if len(parts) == 3:
                        row = int(parts[0])
                        col = int(parts[1])
                        count = int(parts[2])
                        matrix_sum[(row, col)] += count
            
            # Print progress for every file
            elapsed = time.time() - start_time
            rate = idx / elapsed if elapsed > 0 else 0
            remaining = (total_matrices - idx) / rate if rate > 0 else 0
            
            # Use carriage return to overwrite line, newline every 10 files
            end_char = '\n' if (idx % 10 == 0 or idx == total_matrices) else '\r'
            
            print(f"  Progress: {idx}/{total_matrices} ({100*idx/total_matrices:.1f}%) | "
                  f"Elapsed: {elapsed:.1f}s | Rate: {rate:.2f} files/s | "
                  f"ETA: {remaining:.1f}s | Pairs: {len(matrix_sum)}", 
                  end=end_char, flush=True)
        
        print(f"  Total unique (row, col) pairs: {len(matrix_sum)}")
        
        # Write combined matrix
        print(f"Writing combined matrix to: {output.combined}")
        with open(output.combined, 'w') as f:
            # Sort by row, then by col for consistent output
            for (row, col), count in sorted(matrix_sum.items()):
                f.write(f"{row}\t{col}\t{count}\n")
        
        print("Matrix summation complete")

rule plot_heterozygosity:
    """
    Generate heterozygosity plots from the combined CHEP matrix.
    Creates all output plots in the plots subdirectory.
    """
    input:
        matrix = WORKDIR + "/chep_combined_matrix.txt",
        genome = REF
    output:
        depth_tsv    = WORKDIR + "/plots/plots_depth_num_bases.tsv",
        het_tsv      = WORKDIR + "/plots/plots_het_by_position.tsv",
        main_pdf     = WORKDIR + "/plots/plots_marginal_het_het_plot.pdf",
        main_png     = WORKDIR + "/plots/plots_marginal_het_het_plot.png",
        marginal_pdf = WORKDIR + "/plots/plots_marginal_het_plot.pdf",
        marginal_png = WORKDIR + "/plots/plots_marginal_het_plot.png",
        simple_pdf   = WORKDIR + "/plots/plots_simple_het_plot.pdf",
        simple_png   = WORKDIR + "/plots/plots_simple_het_plot.png"
    params:
        outprefix = WORKDIR + "/plots/plots"
    threads: 1
    resources:
        mem_mb = 8000,
        runtime = 10
    shell:
        """
        # Create plots directory
        mkdir -p {params.outprefix:q}/../
        
        # Run plotting script with auto-detection of x-axis range
        {CHEP_BIN}/chep_plot \\
            -f {input.matrix} \\
            -g {input.genome} \\
            -o {params.outprefix}
        
        echo "Plots generated in: $(dirname {params.outprefix})"
        """

rule windowed_depth_stats:
    """
    Calculate depth statistics in 50kbp windows from a pileup file.
    Output: scaf_name, start, stop, depth_mode, depth_mean, num_positions_with_reads
    Runtime scales with pileup.gz file size (15 min/GB with 2x safety margin, min 30 min)
    """
    input:
        pileup = WORKDIR + "/pileups/{chrom}.pileup.gz"
    output:
        stats = WORKDIR + "/depth_stats/{chrom}_depth_stats.tsv"
    params:
        window_size = 50000
    threads: 1
    resources:
        mem_mb = 2000,
        runtime = lambda wildcards, input, attempt: estimate_depth_stats_runtime(input.pileup, attempt)
    run:
        import gzip
        from collections import defaultdict
        import statistics
        
        window_size = params.window_size
        
        # Dictionary to store depths per window: (chrom, window_start) -> [depths]
        windows = defaultdict(list)
        
        print(f"Processing pileup: {input.pileup}")
        
        # Read pileup file
        with gzip.open(input.pileup, 'rt') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) < 4:
                    continue
                
                chrom = parts[0]
                pos = int(parts[1])
                depth = int(parts[3])
                
                # Calculate which window this position belongs to
                window_start = (pos // window_size) * window_size
                
                windows[(chrom, window_start)].append(depth)
        
        print(f"Found {len(windows)} windows with coverage")
        
        # Calculate statistics for each window and write output
        with open(output.stats, 'w') as out:
            # Write header
            out.write("scaf_name\tstart\tstop\tdepth_mode\tdepth_mean\tnum_positions_with_reads\n")
            
            # Sort windows by chromosome and position
            for (chrom, window_start) in sorted(windows.keys()):
                depths = windows[(chrom, window_start)]
                window_stop = window_start + window_size
                
                # Calculate statistics
                mean_depth = statistics.mean(depths)
                
                # Calculate mode (most common depth value)
                # Use a simple frequency count
                depth_counts = defaultdict(int)
                for d in depths:
                    depth_counts[d] += 1
                mode_depth = max(depth_counts.keys(), key=lambda k: depth_counts[k])
                
                num_positions = len(depths)
                
                # Write output
                out.write(f"{chrom}\t{window_start}\t{window_stop}\t"
                         f"{mode_depth}\t{mean_depth:.2f}\t{num_positions}\n")
        
        print(f"Depth statistics written to: {output.stats}")

## Keep the old rule commented out for reference
#rule mpileup_bam:
#    """
#    Run samtools mpileup on a single BAM file.
#    Output gzipped pileup text.
#    Uses local tmpdir for I/O efficiency.
#    """
#    input:
#        bam    = WORKDIR + "/merged_all.bam",
#        bai    = WORKDIR + "/merged_all.bam.bai",
#        ref    = REF
#    output:
#        pileup = WORKDIR + "/merged_all.pileup.gz"
#    threads: 1
#    resources:
#        mem_mb = estimate_mem_mb_for_bam,
#        runtime = estimate_runtime_for_bam
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_mpileup_XXXXXX")
#        
#        echo "Running mpileup on {params.bam_id}"
#        
#        # Copy BAM to local tmpdir
#        LOCAL_BAM="$LOCAL_TMP/input.bam"
#        cp {input.bam} "$LOCAL_BAM"
#
#        # Copy or create index
#        if [ -f {input.bam}.bai ]; then
#            cp {input.bam}.bai "$LOCAL_BAM.bai"
#        else
#            samtools index "$LOCAL_BAM"
#        fi
#        
#        # Run mpileup and gzip to tmpdir
#        LOCAL_OUT="$LOCAL_TMP/output.pileup.gz"
#        samtools mpileup -B -Q 0 -q 0 -A -R -f {params.ref} "$LOCAL_BAM" | gzip > "$LOCAL_OUT"
#        
#        # Copy result back
#        mkdir -p $(dirname {output.pileup})
#        cp "$LOCAL_OUT" {output.pileup}
#        
#        # Cleanup
#        rm -rf "$LOCAL_TMP"
#        
#        echo "Completed mpileup: {params.bam_id}"
#        """

