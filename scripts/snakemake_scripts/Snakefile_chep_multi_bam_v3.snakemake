"""
Snakemake workflow for hierarchical BAM merging (v3).
Strategy: Two-level merge using sqrt(N) batches for balanced merging.

Level 1: Merge N BAMs into sqrt(N) batch BAMs
Level 2: Merge sqrt(N) batch BAMs into 1 final BAM

Usage:
    snakemake -s Snakefile_chep_multi_bam_v3.snakemake \
        --config ref=reference.fa bam_list=bam_list.txt output=merged.bam \
        --executor slurm --jobs 100
"""

import os
import math
from pathlib import Path

# Get config parameters
REF = config.get("ref", "reference.fa")
BAM_LIST = config.get("bam_list", "bam_list.txt")
OUTPUT = config.get("output", "merged_all.bam")
WORKDIR = config.get("workdir", "chep_merge_work")

# Get the directory containing this Snakefile
SNAKEFILE_DIR = Path(workflow.basedir)

# Read BAM file list
with open(BAM_LIST) as f:
    BAMS = [line.strip() for line in f if line.strip()]

NUM_BAMS = len(BAMS)

# Calculate optimal batch size (square root of total BAMs)
BATCH_SIZE = max(2, int(math.sqrt(NUM_BAMS)))
NUM_BATCHES = math.ceil(NUM_BAMS / BATCH_SIZE)

print(f"Total BAMs: {NUM_BAMS}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Number of batches: {NUM_BATCHES}")

BAM_IDS = [f"bam_{i:04d}" for i in range(NUM_BAMS)]
BATCH_IDS = [f"batch_{i:04d}" for i in range(NUM_BATCHES)]

def get_bams_for_batch(batch_id):
    """Get list of BAM files for a given batch"""
    batch_num = int(batch_id.split("_")[1])
    start_idx = batch_num * BATCH_SIZE
    end_idx = min(start_idx + BATCH_SIZE, NUM_BAMS)
    return BAMS[start_idx:end_idx]

def estimate_final_merge_runtime(attempt=1):
    """
    Estimate runtime for final merge based on number of BAMs.
    
    Empirical observation: 768 BAMs takes ~9 hours (540 minutes) for merge only.
    (Indexing is done separately in its own rule)
    
    This includes:
    - Copying batch BAMs to tmpdir
    - Merging all batch BAMs
    - Copying back to network storage
    
    Runtime scales approximately linearly with number of BAMs.
    Formula: (NUM_BAMS / 768) × 540 minutes × 1.4 safety margin
    """
    # Base runtime for 768 BAMs is 540 minutes (9 hours)
    base_runtime = (NUM_BAMS / 768.0) * 540
    
    # Add 40% safety margin
    safe_runtime = int(base_runtime * 1.4)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_index_runtime(attempt=1):
    """
    Estimate runtime for indexing final merged BAM based on number of BAMs.
    
    Empirical observation: Indexing 768 merged BAMs takes longer than initially expected.
    
    Runtime scales with the size of the final merged BAM, which is roughly
    proportional to the number of input BAMs.
    """
    # Base runtime for 768 BAMs is ~360 minutes (6 hours) for indexing
    base_runtime = (NUM_BAMS / 768.0) * 360
    
    # Add 50% safety margin
    safe_runtime = int(base_runtime * 1.5)
    
    # Scale with retry attempts
    return safe_runtime * attempt

def estimate_split_runtime(bam_path, attempt=1):
    """
    Estimate runtime for splitting BAM by chromosome based on file size.
    
    Empirical observation: 123GB BAM file failed at 4 hours (240 min) on first attempt,
    succeeded on second attempt (480 min = 8 hours).
    
    Runtime scales with BAM file size.
    Formula: (file_size_gb / 123) × 480 minutes × 1.3 safety margin
    """
    import os
    
    # Get file size in GB
    file_size_bytes = os.path.getsize(bam_path)
    file_size_gb = file_size_bytes / (1024**3)
    
    # Base runtime for 123GB is 480 minutes (8 hours)
    base_runtime = (file_size_gb / 123.0) * 480
    
    # Add 30% safety margin
    safe_runtime = int(base_runtime * 1.3)
    
    # Scale with retry attempts
    return safe_runtime * attempt

# Create work directories
os.makedirs(WORKDIR, exist_ok=True)
os.makedirs(WORKDIR + "/batch_bams", exist_ok=True)

# Wildcard constraints
wildcard_constraints:
    batch_id = r"batch_\d+",
    chrom = r"[A-Za-z0-9_.\-]+"

def get_chrom_pileups(wildcards):
    """
    Aggregation function to get list of chromosome pileup files.
    This is called after split_bam_by_chromosome checkpoint completes,
    allowing Snakemake to dynamically determine which chromosomes were created.
    """
    checkpoint_output = checkpoints.split_bam_by_chromosome.get(**wildcards).output.done
    outdir = WORKDIR + "/split_by_chrom"
    
    # Get all .bam files in the split directory
    import glob
    bam_files = glob.glob(outdir + "/*.bam")
    
    # Extract chromosome names from filenames
    chroms = [os.path.basename(f).replace(".bam", "") for f in bam_files]
    
    # Return list of pileup files to generate
    return expand(WORKDIR + "/pileups/{chrom}.pileup.gz", chrom=chroms)

rule all:
    input:
        get_chrom_pileups

rule merge_batch_bams:
    """
    Merge a batch of BAMs into a single batch BAM.
    Uses local tmpdir for fast I/O.
    """
    input:
        bams = lambda wc: get_bams_for_batch(wc.batch_id)
    output:
        bam = WORKDIR + "/batch_bams/{batch_id}.bam",
        bai = WORKDIR + "/batch_bams/{batch_id}.bam.bai"
    params:
        batch_id = "{batch_id}",
        num_bams = lambda wc: len(get_bams_for_batch(wc.batch_id))
    threads: 4
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: max(8000, len(get_bams_for_batch(wildcards.batch_id)) * 500) * attempt,
        runtime = lambda wildcards, attempt: 120 * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_batch_merge_XXXXXX")
        mkdir -p "$LOCAL_TMP/bams"

        echo "Merging {params.batch_id} with {params.num_bams} BAMs..."
        echo "Working directory: $LOCAL_TMP"

        # Copy BAMs to local tmpdir with unique names
        BAM_LIST="$LOCAL_TMP/bam_list.txt"
        counter=0
        for bam in {input.bams}; do
            counter=$((counter + 1))
            LOCAL_BAM="$LOCAL_TMP/bams/bam_${{counter}}.bam"
            echo "  Copying BAM $counter to tmpdir..."
            cp "$bam" "$LOCAL_BAM"

            # Copy or create index
            if [ -f "${{bam}}.bai" ]; then
                cp "${{bam}}.bai" "${{LOCAL_BAM}}.bai"
            else
                echo "  Creating index for BAM $counter..."
                samtools index "$LOCAL_BAM"
            fi

            echo "$LOCAL_BAM" >> "$BAM_LIST"
        done

        # Merge BAMs
        echo "Merging {params.num_bams} BAMs in {params.batch_id}..."
        LOCAL_OUT="$LOCAL_TMP/merged.bam"
        samtools merge -@ {threads} -b "$BAM_LIST" "$LOCAL_OUT"

        # Index the merged BAM
        echo "Indexing merged BAM..."
        samtools index "$LOCAL_OUT"

        # Copy results back
        echo "Copying merged BAM to output directory..."
        mkdir -p $(dirname {output.bam})
        cp "$LOCAL_OUT" {output.bam}
        cp "${{LOCAL_OUT}}.bai" {output.bai}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Completed merge for {params.batch_id}"
        """

rule merge_all_batches:
    """
    Merge all batch BAMs into final output BAM.
    Uses local tmpdir for fast I/O.
    Does NOT index - that's done in a separate rule.
    """
    input:
        bams = expand(WORKDIR + "/batch_bams/{batch_id}.bam", batch_id=BATCH_IDS),
        bais = expand(WORKDIR + "/batch_bams/{batch_id}.bam.bai", batch_id=BATCH_IDS)
    output:
        bam = WORKDIR + "/merged_all.bam"
    params:
        num_batches = NUM_BATCHES
    threads: 8
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: max(16000, NUM_BATCHES * 1000) * attempt,
        runtime = lambda wildcards, attempt: estimate_final_merge_runtime() * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_final_merge_XXXXXX")
        mkdir -p "$LOCAL_TMP/bams"

        echo "Merging {params.num_batches} batch BAMs into final output..."
        echo "Working directory: $LOCAL_TMP"

        # Copy batch BAMs to local tmpdir
        BAM_LIST="$LOCAL_TMP/bam_list.txt"
        counter=0
        for bam in {input.bams}; do
            counter=$((counter + 1))
            LOCAL_BAM="$LOCAL_TMP/bams/batch_${{counter}}.bam"
            echo "  Copying batch BAM $counter to tmpdir..."
            cp "$bam" "$LOCAL_BAM"
            cp "${{bam}}.bai" "${{LOCAL_BAM}}.bai"
            echo "$LOCAL_BAM" >> "$BAM_LIST"
        done

        # Final merge (NO INDEXING - that's done separately)
        echo "Merging {params.num_batches} batch BAMs..."
        LOCAL_OUT="$LOCAL_TMP/final_merged.bam"
        samtools merge -@ {threads} -b "$BAM_LIST" "$LOCAL_OUT"

        # Copy results back
        echo "Copying final merged BAM to output directory..."
        mkdir -p $(dirname {output.bam})
        cp "$LOCAL_OUT" {output.bam}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Final merge complete: {output.bam}"
        """

rule index_final_bam:
    """
    Index the final merged BAM.
    Separate from merge to allow better resource allocation.
    Note: samtools index uses 2 hidden threads, so we reserve 10 threads
    from SLURM but only tell samtools to use 8.
    """
    input:
        bam = WORKDIR + "/merged_all.bam"
    output:
        bai = WORKDIR + "/merged_all.bam.bai"
    threads: 10
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 4000 * attempt,
        runtime = lambda wildcards, attempt: estimate_index_runtime() * attempt
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_index_XXXXXX")
        
        echo "Indexing final merged BAM: {input.bam}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy BAM to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/merged.bam"
        echo "Copying BAM to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        
        # Index the BAM (use 8 threads, SLURM reserves 10 for the 2 hidden threads)
        echo "Indexing BAM..."
        samtools index -@ 8 "$LOCAL_BAM"
        
        # Copy index back to output
        echo "Copying index to output directory..."
        mkdir -p $(dirname {output.bai})
        cp "${{LOCAL_BAM}}.bai" {output.bai}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Indexing complete: {output.bai}"
        """

checkpoint split_bam_by_chromosome:
    """
    Split the final merged BAM by chromosome.
    Uses GNU parallel to split in parallel (4 jobs at a time).
    Only processes chromosomes >= 50kbp in length.
    Each chromosome gets its own BAM file and index.
    
    Note: Runs 4 parallel jobs, each using samtools with 4 threads = 16 threads total.
    With observed ~20-30% CPU utilization, we reserve 20 threads to be safe.
    """
    input:
        bam = WORKDIR + "/merged_all.bam",
        bai = WORKDIR + "/merged_all.bam.bai"
    output:
        done = WORKDIR + "/split_by_chrom/split.done"
    params:
        outdir = WORKDIR + "/split_by_chrom"
    threads: 20
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 8000 * attempt,
        runtime = lambda wildcards, attempt, input: estimate_split_runtime(input.bam, attempt)
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_split_XXXXXX")
        
        echo "Splitting BAM by chromosome (only >= 50kbp): {input.bam}"
        echo "Working directory: $LOCAL_TMP"
        
        # Copy BAM and index to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/merged.bam"
        echo "Copying BAM and index to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        cp {input.bai} "${{LOCAL_BAM}}.bai"
        
        # Create local output directory
        LOCAL_OUTDIR="$LOCAL_TMP/split_chroms"
        mkdir -p "$LOCAL_OUTDIR"
        
        # Split by chromosome using parallel (only chromosomes >= 50000 bp)
        # Run 4 parallel jobs, each using 4 threads (16 threads total)
        echo "Splitting by chromosome (4 parallel jobs, filtering >= 50kbp)..."
        samtools idxstats "$LOCAL_BAM" | awk '$1!="*" && $2>=50000{{print $1}}' | \
          parallel -j 4 "samtools view -@ 4 -b $LOCAL_BAM {{}} -o $LOCAL_OUTDIR/{{}}.bam && samtools index -@ 4 $LOCAL_OUTDIR/{{}}.bam"
        
        # Copy results back to output directory
        echo "Copying split BAMs to output directory..."
        mkdir -p {params.outdir}
        cp "$LOCAL_OUTDIR"/*.bam {params.outdir}/
        cp "$LOCAL_OUTDIR"/*.bai {params.outdir}/
        
        # Create done marker
        touch {output.done}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Chromosome splitting complete: {params.outdir}"
        """


rule mpileup_chrom:
    """
    Run samtools mpileup on a single chromosome BAM file.
    Output gzipped pileup text.
    Uses local tmpdir for I/O efficiency.
    
    Each chromosome is processed independently for parallelization.
    """
    input:
        bam = WORKDIR + "/split_by_chrom/{chrom}.bam",
        bai = WORKDIR + "/split_by_chrom/{chrom}.bam.bai",
        ref = REF
    output:
        pileup = WORKDIR + "/pileups/{chrom}.pileup.gz"
    threads: 1
    retries: 2
    resources:
        mem_mb = lambda wildcards, attempt: 4000 * attempt,
        runtime = lambda wildcards, attempt: 120 * attempt  # 2 hours base
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_mpileup_XXXXXX")

        echo "Running mpileup on chromosome {wildcards.chrom}"
        echo "Working directory: $LOCAL_TMP"

        # Copy BAM and index to local tmpdir for fast I/O
        LOCAL_BAM="$LOCAL_TMP/{wildcards.chrom}.bam"
        echo "Copying BAM and index to tmpdir..."
        cp {input.bam} "$LOCAL_BAM"
        cp {input.bai} "${{LOCAL_BAM}}.bai"

        # Run mpileup and pipe directly to gzip in tmpdir
        LOCAL_OUT="$LOCAL_TMP/{wildcards.chrom}.pileup.gz"
        echo "Generating pileup..."
        samtools mpileup -B -Q 0 -q 0 -A -R -f {input.ref} "$LOCAL_BAM" | gzip > "$LOCAL_OUT"

        # Copy gzipped result back to output directory
        echo "Copying pileup to output directory..."
        mkdir -p $(dirname {output.pileup})
        cp "$LOCAL_OUT" {output.pileup}

        # Cleanup
        rm -rf "$LOCAL_TMP"

        echo "Completed mpileup for chromosome {wildcards.chrom}"
        """



## Keep the old rule commented out for reference
#rule mpileup_bam:
#    """
#    Run samtools mpileup on a single BAM file.
#    Output gzipped pileup text.
#    Uses local tmpdir for I/O efficiency.
#    """
#    input:
#        bam    = WORKDIR + "/merged_all.bam",
#        bai    = WORKDIR + "/merged_all.bam.bai",
#        ref    = REF
#    output:
#        pileup = WORKDIR + "/merged_all.pileup.gz"
#    threads: 1
#    resources:
#        mem_mb = estimate_mem_mb_for_bam,
#        runtime = estimate_runtime_for_bam
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_mpileup_XXXXXX")
#        
#        echo "Running mpileup on {params.bam_id}"
#        
#        # Copy BAM to local tmpdir
#        LOCAL_BAM="$LOCAL_TMP/input.bam"
#        cp {input.bam} "$LOCAL_BAM"
#
#        # Copy or create index
#        if [ -f {input.bam}.bai ]; then
#            cp {input.bam}.bai "$LOCAL_BAM.bai"
#        else
#            samtools index "$LOCAL_BAM"
#        fi
#        
#        # Run mpileup and gzip to tmpdir
#        LOCAL_OUT="$LOCAL_TMP/output.pileup.gz"
#        samtools mpileup -B -Q 0 -q 0 -A -R -f {params.ref} "$LOCAL_BAM" | gzip > "$LOCAL_OUT"
#        
#        # Copy result back
#        mkdir -p $(dirname {output.pileup})
#        cp "$LOCAL_OUT" {output.pileup}
#        
#        # Cleanup
#        rm -rf "$LOCAL_TMP"
#        
#        echo "Completed mpileup: {params.bam_id}"
#        """

