"""
Snakemake workflow to process multiple BAM files for CHEP analysis.
Strategy: Split by chromosome, merge per-chromosome BAMs, run mpileup per chromosome, merge results.

Usage:
    snakemake -s Snakefile_chep_multi_bam_v2.snakemake \
        --config ref=reference.fa bam_list=bam_list.txt output=chep_3D.txt \
        --executor slurm --jobs 100
"""

import os
import gzip
import glob
from pathlib import Path

# Get config parameters
REF = config.get("ref", "reference.fa")
BAM_LIST = config.get("bam_list", "bam_list.txt")
OUTPUT = config.get("output", "chep_3D.txt")
WORKDIR = config.get("workdir", "chep_work")
MIN_CHROM_SIZE = config.get("min_chrom_size_mbp", 1.0) * 1_000_000  # Default 1 Mbp
GENOME_SIZE_GB = config.get("genome_size_gb", 3.0)

# Get the directory containing this Snakefile
SNAKEFILE_DIR = Path(workflow.basedir)
CHEP_BIN = SNAKEFILE_DIR.parent.parent / "bin"
CHEP_SCRIPTS = SNAKEFILE_DIR.parent

# Path to executables
CHEP_PILEUP_TO_ARRAY = CHEP_BIN / "chep_pileup_to_array"
CHEP_PLOT = CHEP_BIN / "chep_plot"

# Read BAM file list
with open(BAM_LIST) as f:
    BAMS = [line.strip() for line in f if line.strip()]

BAM_IDS = [f"bam_{i:04d}" for i in range(len(BAMS))]
BAM_DICT = dict(zip(BAM_IDS, BAMS))

# Create work directories
os.makedirs(WORKDIR,                  exist_ok=True)
os.makedirs(f"{WORKDIR}/split_bams",  exist_ok=True)
os.makedirs(f"{WORKDIR}/merged_bams", exist_ok=True)
os.makedirs(f"{WORKDIR}/pileups",     exist_ok=True)

def get_reference_size_gb():
    """Get reference genome file size in GB"""
    try:
        size_bytes = os.path.getsize(REF)
        return size_bytes / (1024 * 1024 * 1024)
    except:
        return GENOME_SIZE_GB

def estimate_mem_mb_base(attempt=1):
    """Base memory estimate for mpileup based on reference size"""
    ref_size_gb = get_reference_size_gb()
    base_mem = max(4000, int(ref_size_gb * 2000) + 2000)
    return base_mem * attempt

def estimate_mem_mb_split(wildcards, attempt=1):
    """Memory for splitting BAMs - relatively light"""
    return 2000 * attempt

def estimate_mem_mb_merge(wildcards, input, attempt=1):
    """Memory for merging BAMs - scales with number of input files"""
    num_files = len(input)
    base_mem = max(4000, 2000 + num_files * 100)
    return min(base_mem * attempt, 32000)

def estimate_mem_mb_mpileup(wildcards, attempt=1):
    """Memory for mpileup - based on reference size"""
    return estimate_mem_mb_base(attempt)

def estimate_runtime_split(wildcards, attempt=1):
    """Runtime for splitting BAMs - depends on BAM size"""
    try:
        bam_path = BAM_DICT[wildcards.bam_id]
        size_mb = os.path.getsize(bam_path) / (1024 * 1024)
        base_time = max(20, int(size_mb * 0.05))  # ~3 seconds per MB
    except:
        base_time = 30
    return base_time * attempt

def estimate_runtime_merge(wildcards, input, attempt=1):
    """Runtime for merging BAMs - scales with number of files"""
    num_files = len(input)
    base_time = max(30, int(num_files * 2))  # ~2 min per file
    return base_time * attempt

def estimate_runtime_mpileup(wildcards, attempt=1):
    """Runtime for mpileup - depends on chromosome size and number of cells"""
    num_bams = len(BAMS)
    base_time = max(60, int(num_bams * 0.5))  # ~30 seconds per BAM
    return base_time * attempt

def estimate_mem_mb_for_bam(wildcards, attempt=1):
    """
    Memory estimate for mpileup on a single BAM.
    Based on reference genome size and BAM file size.
    - Reference genome size affects memory for loading reference
    - BAM size affects memory for caching and processing
    """
    try:
        # Get reference size in GB
        ref_size_gb = get_reference_size_gb()
        
        # Get BAM size in MB
        bam_path = BAM_DICT[wildcards.bam_id]
        bam_size_mb = os.path.getsize(bam_path) / (1024 * 1024)
        
        # Base memory: ~500MB per GB of reference, min 1GB
        ref_mem_mb = max(1000, int(ref_size_gb * 500))
        
        # Add memory for BAM processing: ~2MB per 100MB of BAM
        bam_mem_mb = int(bam_size_mb * 0.02)
        
        # Add buffer for decompression and processing (~300MB)
        base_mem = ref_mem_mb + bam_mem_mb + 300
        
        # Add 30% safety margin and scale with attempts
        return int(base_mem * 1.3) * attempt
    except:
        # Fallback to conservative estimate
        return 2000 * attempt

def estimate_runtime_for_bam(wildcards, attempt=1):
    """
    Runtime estimate for mpileup on a single BAM.
    Based on BOTH BAM file size AND reference genome size.
    Empirical observation: ~5 minutes for 283MB BAM with 3GB reference
    
    Runtime depends on:
    - BAM size: more reads to process
    - Reference size: more positions to evaluate
    """
    try:
        # Get BAM size in MB
        bam_path = BAM_DICT[wildcards.bam_id]
        bam_size_mb = os.path.getsize(bam_path) / (1024 * 1024)
        
        # Get reference size in GB
        ref_size_gb = get_reference_size_gb()
        
        # Mpileup processes every position in the reference, checking BAM for coverage
        # Time scales with: (BAM size) × (reference size)
        # Empirical: 283MB BAM × 3GB ref = 5 minutes
        # So: time ≈ (bam_mb × ref_gb) / (283 × 3) × 5
        
        baseline_product = 283 * 3  # 849 MB*GB from empirical test
        baseline_time = 5  # minutes
        
        current_product = bam_size_mb * ref_size_gb
        base_time = (current_product / baseline_product) * baseline_time
        
        # Ensure minimum time
        base_time = max(10, int(base_time))
        
        # Add 100% safety margin (double the time) to handle variations
        # in read depth, reference complexity, etc.
        safe_time = base_time * 2
        
        # Scale with retry attempts
        return int(safe_time) * attempt
    except Exception as e:
        # Fallback to conservative estimate
        return 30 * attempt


# limit the bam_id wildcard
wildcard_constraints:
    bam_id = r"bam_\d+"

rule all:
    input:
        expand(WORKDIR + "/pileups/{bam_id}.pileup.gz", bam_id=BAM_IDS)

rule mpileup_bam:
    """
    Run samtools mpileup on a single BAM file.
    Output gzipped pileup text.
    Uses local tmpdir for I/O efficiency.
    """
    input:
        bam = lambda wc: BAM_DICT[wc.bam_id]
    output:
        pileup = WORKDIR + "/pileups/{bam_id}.pileup.gz"
    params:
        ref = REF,
        bam_id = "{bam_id}"
    threads: 1
    resources:
        mem_mb = estimate_mem_mb_for_bam,
        runtime = estimate_runtime_for_bam
    shell:
        """
        TMPDIR=${{TMPDIR:-/tmp}}
        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_mpileup_XXXXXX")
        
        echo "Running mpileup on {params.bam_id}"
        
        # Copy BAM to local tmpdir
        LOCAL_BAM="$LOCAL_TMP/input.bam"
        cp {input.bam} "$LOCAL_BAM"
        
        # Copy or create index
        if [ -f {input.bam}.bai ]; then
            cp {input.bam}.bai "$LOCAL_BAM.bai"
        else
            samtools index "$LOCAL_BAM"
        fi
        
        # Run mpileup and gzip to tmpdir
        LOCAL_OUT="$LOCAL_TMP/output.pileup.gz"
        samtools mpileup -B -Q 0 -q 0 -A -R -f {params.ref} "$LOCAL_BAM" | gzip > "$LOCAL_OUT"
        
        # Copy result back
        mkdir -p $(dirname {output.pileup})
        cp "$LOCAL_OUT" {output.pileup}
        
        # Cleanup
        rm -rf "$LOCAL_TMP"
        
        echo "Completed mpileup: {params.bam_id}"
        """

##rule merge_all_bams:
#    """
#    Merge ALL 768 BAMs into one master BAM.
#    Uses local tmpdir for I/O efficiency.
#    """
#    input:
#        bams = BAMS
#    output:
#        bam = WORKDIR + "/merged_all.bam"
#    params:
#        num_bams = len(BAMS)
#    threads: 4
#    resources:
#        mem_mb = lambda wildcards, attempt: max(8000, len(BAMS) * 100) * attempt,
#        runtime = lambda wildcards, attempt: max(120, len(BAMS) * 2) * attempt
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_merge_all_XXXXXX")
#
#        echo "Merging {params.num_bams} BAMs into one master BAM"
#        echo "Copying BAMs to local tmpdir: $LOCAL_TMP"
#
#        # Create directory for input BAMs
#        mkdir -p "$LOCAL_TMP/inputs"
#
#        # Copy all BAMs to local tmpdir with unique names and create list
#        BAM_LIST="$LOCAL_TMP/bam_list.txt"
#        counter=0
#        for bam in {input.bams}; do
#            counter=$((counter + 1))
#            if [ $((counter % 100)) -eq 0 ]; then
#                echo "  Copied $counter BAMs..."
#            fi
#            # Use counter to ensure unique filenames
#            local_bam="$LOCAL_TMP/inputs/bam_${{counter}}.bam"
#            cp "$bam" "$local_bam"
#            echo "$local_bam" >> "$BAM_LIST"
#        done
#        echo "Finished copying {params.num_bams} BAMs to tmpdir"
#
#        # Merge all BAMs
#        echo "Starting merge..."
#        LOCAL_OUT="$LOCAL_TMP/merged.bam"
#        samtools merge -@ {threads} -b "$BAM_LIST" "$LOCAL_OUT"
#
#        # Copy result back
#        echo "Copying merged BAM back to working directory..."
#        cp "$LOCAL_OUT" {output.bam}
#
#        # Cleanup
#        rm -rf "$LOCAL_TMP"
#
#        echo "Completed merge of all BAMs"
#        """
#
#rule sort_merged_bam:
#    """
#    Sort the merged BAM with 32 threads.
#    """
#    input:
#        bam = WORKDIR + "/merged_all.bam"
#    output:
#        bam = WORKDIR + "/merged_all.sorted.bam",
#        bai = WORKDIR + "/merged_all.sorted.bam.bai"
#    threads: 32
#    resources:
#        mem_mb = lambda wildcards, attempt: 64000 * attempt,
#        runtime = lambda wildcards, attempt: 240 * attempt
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_sort_XXXXXX")
#        
#        echo "Sorting merged BAM with {threads} threads"
#        
#        # Copy to local tmpdir
#        LOCAL_BAM="$LOCAL_TMP/input.bam"
#        cp {input.bam} "$LOCAL_BAM"
#        
#        # Sort
#        LOCAL_OUT="$LOCAL_TMP/sorted.bam"
#        samtools sort -@ {threads} -m 2G -T "$LOCAL_TMP/sort_tmp" "$LOCAL_BAM" -o "$LOCAL_OUT"
#        
#        # Index
#        samtools index "$LOCAL_OUT"
#        
#        # Copy results back
#        cp "$LOCAL_OUT" {output.bam}
#        cp "$LOCAL_OUT.bai" {output.bai}
#        
#        # Cleanup
#        rm -rf "$LOCAL_TMP"
#        
#        echo "Completed sorting"
#        """
#
#checkpoint get_chromosomes:
#    """
#    Identify chromosomes >= min size from the sorted merged BAM.
#    Creates one output file per chromosome so downstream rules can use {chrom} wildcard.
#    """
#    input:
#        bam = WORKDIR + "/merged_all.sorted.bam",
#        bai = WORKDIR + "/merged_all.sorted.bam.bai"
#    output:
#        chroms_dir = directory(WORKDIR + "/chromosomes")
#    shell:
#        """
#        mkdir -p {output.chroms_dir}
#
#        # Get chromosome names and lengths from BAM header
#        samtools idxstats {input.bam} | awk -v minsize={MIN_CHROM_SIZE} '$2 >= minsize && $1 != "*" {{print $1}}' | while read chrom; do
#            echo "Selected: $chrom"
#            touch {output.chroms_dir}/$chrom.txt
#        done
#
#        echo "Chromosome selection complete"
#        ls {output.chroms_dir}/
#        """
#
#rule split_merged_bam_by_chromosome:
#    """
#    Split the merged sorted BAM into one BAM per chromosome.
#    One rule per chromosome - much simpler than before!
#    """
#    input:
#        bam = WORKDIR + "/merged_all.sorted.bam",
#        bai = WORKDIR + "/merged_all.sorted.bam.bai",
#        chrom_marker = WORKDIR + "/chromosomes/{chrom}.txt"
#    output:
#        bam = WORKDIR + "/split_bams/{chrom}.bam",
#        bai = WORKDIR + "/split_bams/{chrom}.bam.bai"
#    threads: 4
#    resources:
#        mem_mb = lambda wildcards, attempt: 4000 * attempt,
#        runtime = lambda wildcards, attempt: 60 * attempt
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d "$TMPDIR/chep_split_chrom_XXXXXX")
#        
#        echo "Extracting chromosome {wildcards.chrom}"
#        
#        # Copy merged BAM to local tmpdir
#        LOCAL_BAM="$LOCAL_TMP/merged.bam"
#        cp {input.bam} "$LOCAL_BAM"
#        cp {input.bai} "$LOCAL_BAM.bai"
#        
#        # Extract this chromosome
#        LOCAL_OUT="$LOCAL_TMP/{wildcards.chrom}.bam"
#        samtools view -@ {threads} -b "$LOCAL_BAM" "{wildcards.chrom}" > "$LOCAL_OUT"
#        
#        # Index
#        samtools index "$LOCAL_OUT"
#        
#        # Copy results back
#        mkdir -p $(dirname {output.bam})
#        cp "$LOCAL_OUT" {output.bam}
#        cp "$LOCAL_OUT.bai" {output.bai}
#        
#        # Cleanup
#        rm -rf "$LOCAL_TMP"
#        
#        echo "Completed: {wildcards.chrom}"
#        """

#rule mpileup_chromosome:
#    """
#    Run samtools mpileup on chromosome BAM.
#    Output gzipped pileup text.
#    """
#    input:
#        bam = WORKDIR + "/split_bams/{chrom}.bam",
#        bai = WORKDIR + "/split_bams/{chrom}.bam.bai"
#    output:
#        WORKDIR + "/pileups/{chrom}.pileup.gz"
#    params:
#        ref = REF
#    threads: 1
#    resources:
#        mem_mb = estimate_mem_mb_mpileup,
#        runtime = estimate_runtime_mpileup
#    shell:
#        """
#        TMPDIR=${{TMPDIR:-/tmp}}
#        LOCAL_TMP=$(mktemp -d $TMPDIR/chep_mpileup_XXXXXX)
#        
#        echo "Running mpileup on {wildcards.chrom}"
#        
#        # Copy BAM and index to local tmpdir
#        LOCAL_BAM=$LOCAL_TMP/input.bam
#        cp {input.bam} $LOCAL_BAM
#        cp {input.bai} $LOCAL_BAM.bai
#        
#        # Run mpileup and gzip
#        LOCAL_OUT=$LOCAL_TMP/output.pileup.gz
#        samtools mpileup -B -Q 0 -q 0 -A -R -f {params.ref} $LOCAL_BAM | gzip > $LOCAL_OUT
#        
#        # Copy result back
#        cp $LOCAL_OUT {output}
#        
#        # Cleanup
#        rm -rf $LOCAL_TMP
#        
#        echo "Completed mpileup: {wildcards.chrom}"
#        """
#
#def get_all_pileups(wildcards):
#    """Get all pileup files after checkpoint resolves"""
#    checkpoint_output = checkpoints.get_chromosomes.get(**wildcards).output.chroms_dir
#    chroms = get_chromosomes_from_checkpoint(wildcards)
#    return expand(WORKDIR + "/pileups/{chrom}.pileup.gz", chrom=chroms)
#
#rule combine_and_process:
#    """
#    Combine all chromosome pileups and process with chep_pileup_to_array.
#    """
#    input:
#        get_all_pileups
#    output:
#        OUTPUT
#    threads: 1
#    resources:
#        mem_mb = 8000,
#        runtime = 60
#    shell:
#        """
#        echo "Combining {len(input)} pileup files and processing with CHEP"
#        
#        # Concatenate all gzipped pileups and pipe to chep
#        zcat {input} | {CHEP_PILEUP_TO_ARRAY} > {output}
#        
#        echo "CHEP processing complete: {output}"
#        """
#
#rule plot_results:
#    """
#    Generate CHEP plots from the processed data.
#    """
#    input:
#        OUTPUT
#    output:
#        OUTPUT.replace(".txt", "_plot.png")
#    shell:
#        """
#        {CHEP_PLOT} -f {input} -x 0 -X 200 -o {output}
#        """
#
#
#rule clean:
#    """Clean up intermediate files"""
#    shell:
#        "rm -rf " + WORKDIR
